{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Next Purchase Date with ML - Top Items"]},{"cell_type":"markdown","metadata":{},"source":["In this use case, we want to know the model's performance if trained with some items with the highest number of transactions. Hopefully, we can achieve the best score because using the largest number of transactions compared to the others will make the model learn and perform better."]},{"cell_type":"markdown","metadata":{},"source":["## Load and Preprocess Data"]},{"cell_type":"markdown","metadata":{},"source":["We use more than 500k transaction data between users and items from the EPM database. The raw data still has some returning transactions with a negative amount, but we are only looking for buying transactions. Each transaction has a timestamp record daily. Because a user can buy the same item multiple times on the same day, we consider it a single data aggregating the sales quantity column."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:27.688737Z","iopub.status.busy":"2024-02-20T05:44:27.688218Z","iopub.status.idle":"2024-02-20T05:44:28.862157Z","shell.execute_reply":"2024-02-20T05:44:28.860953Z","shell.execute_reply.started":"2024-02-20T05:44:27.688704Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, explained_variance_score\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","from xgboost import XGBRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-20T05:44:28.867178Z","iopub.status.busy":"2024-02-20T05:44:28.866320Z","iopub.status.idle":"2024-02-20T05:44:30.310996Z","shell.execute_reply":"2024-02-20T05:44:30.309627Z","shell.execute_reply.started":"2024-02-20T05:44:28.867131Z"},"trusted":true},"outputs":[],"source":["# Load Dataset\n","df = pd.read_csv('/kaggle/input/epm-prep/EPM.csv')\n","df = df.drop(['Unnamed: 0', 'principal_code'], axis=1)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:30.312650Z","iopub.status.busy":"2024-02-20T05:44:30.312308Z","iopub.status.idle":"2024-02-20T05:44:30.515941Z","shell.execute_reply":"2024-02-20T05:44:30.514617Z","shell.execute_reply.started":"2024-02-20T05:44:30.312623Z"},"trusted":true},"outputs":[],"source":["# Filter negative transactions\n","df = df[(df['sales_qty'] > 0) & (df['gross_sales_amount'] > 0)]\n","df['trx_date'] = pd.to_datetime(df['trx_date'])\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:30.519305Z","iopub.status.busy":"2024-02-20T05:44:30.518754Z","iopub.status.idle":"2024-02-20T05:44:31.059670Z","shell.execute_reply":"2024-02-20T05:44:31.058624Z","shell.execute_reply.started":"2024-02-20T05:44:30.519274Z"},"trusted":true},"outputs":[],"source":["# Drop duplicate transactions\n","temp = df[['ship_to_id', 'item_code', 'trx_date', 'sales_qty']].groupby(['ship_to_id', 'item_code', 'trx_date']).sum().reset_index(drop=True)\n","df = df.drop_duplicates(['ship_to_id', 'item_code', 'trx_date']).reset_index(drop=True)\n","df['sales_qty'] = temp\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Preproecssing for Usecase"]},{"cell_type":"markdown","metadata":{},"source":["We don't use all the features but only focus on the sales quantity and timestamp column.\n","\n","In the process of searching the selected data, we do these steps:\n","1. Sort the unique items from the data based on the number of transactions.\n","2. Choose the top x items with the highest number of transactions (in this case, x=1).\n","3. From the selected item, sort the unique users from the data related to the chosen item only.\n","4. Choose the top y users with the highest number of transactions (in this case, y=10)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:31.061654Z","iopub.status.busy":"2024-02-20T05:44:31.061327Z","iopub.status.idle":"2024-02-20T05:44:33.124174Z","shell.execute_reply":"2024-02-20T05:44:33.122816Z","shell.execute_reply.started":"2024-02-20T05:44:31.061626Z"},"trusted":true},"outputs":[],"source":["# Create timestamp column\n","df['timestamp'] = df['trx_date'].apply(lambda x: x.timestamp())\n","df['trx_date'] = pd.to_datetime(df['trx_date'])\n","df = df[['ship_to_id', 'item_code', 'trx_date', 'sales_qty', 'timestamp']]\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:33.125939Z","iopub.status.busy":"2024-02-20T05:44:33.125571Z","iopub.status.idle":"2024-02-20T05:44:33.255098Z","shell.execute_reply":"2024-02-20T05:44:33.253980Z","shell.execute_reply.started":"2024-02-20T05:44:33.125907Z"},"trusted":true},"outputs":[],"source":["# Look for top items with the most transactions \n","dfSortItem = df.groupby(['item_code']).count().sort_values('trx_date', ascending=False).reset_index()\n","dfSortItem[['item_code', 'ship_to_id']]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:33.257618Z","iopub.status.busy":"2024-02-20T05:44:33.256539Z","iopub.status.idle":"2024-02-20T05:44:33.380564Z","shell.execute_reply":"2024-02-20T05:44:33.379379Z","shell.execute_reply.started":"2024-02-20T05:44:33.257575Z"},"trusted":true},"outputs":[],"source":["# Show data with the first top item\n","itemMax = dfSortItem.loc[0, 'item_code']\n","dfItemMax = df[df['item_code'] == itemMax].reset_index(drop=True)\n","dfItemMax"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:33.383031Z","iopub.status.busy":"2024-02-20T05:44:33.382136Z","iopub.status.idle":"2024-02-20T05:44:33.402396Z","shell.execute_reply":"2024-02-20T05:44:33.401194Z","shell.execute_reply.started":"2024-02-20T05:44:33.382997Z"},"trusted":true},"outputs":[],"source":["# Look for top users with most transactions with the related item\n","dfItemMaxSortUser = dfItemMax.groupby(['ship_to_id']).count().sort_values('trx_date', ascending=False).reset_index()\n","dfItemMaxSortUser.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:44:33.404503Z","iopub.status.busy":"2024-02-20T05:44:33.403997Z","iopub.status.idle":"2024-02-20T05:44:33.413384Z","shell.execute_reply":"2024-02-20T05:44:33.412331Z","shell.execute_reply.started":"2024-02-20T05:44:33.404470Z"},"trusted":true},"outputs":[],"source":["top = [i for i in range(10)]\n","usersMax = dfItemMaxSortUser.loc[top, 'ship_to_id']\n","list(usersMax)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Model"]},{"cell_type":"markdown","metadata":{},"source":["After choosing a specific user and item, we put those data into the model. We try three different models and compare the performance. The models are trained with 80% of the data and evaluated with the rest by the Mean Average Error and Mean Squared Error metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T05:46:07.864776Z","iopub.status.busy":"2024-02-20T05:46:07.864330Z","iopub.status.idle":"2024-02-20T05:46:12.562272Z","shell.execute_reply":"2024-02-20T05:46:12.561291Z","shell.execute_reply.started":"2024-02-20T05:46:07.864740Z"},"trusted":true},"outputs":[],"source":["dfTrain = pd.DataFrame()\n","dfTest = pd.DataFrame()\n","\n","for i in usersMax:\n","\n","    dfUser = df[(df['item_code'] == itemMax) & (df['ship_to_id'] == i)].sort_values('trx_date').reset_index(drop=True)\n","    dfUser['period'] = dfUser['trx_date'].diff().apply(lambda x: x.days)[1:].reset_index(drop=True)\n","    dfTrain, dfTest = dfUser[:-1], dfUser[-1:] # The last row doesnt have interval value for next purchase\n","\n","    dfTest = dfTest.drop(['period', 'ship_to_id', 'item_code', 'trx_date'], axis=1)\n","    X = dfTrain.drop(['period', 'ship_to_id', 'item_code', 'trx_date'], axis=1)\n","    y = dfTrain['period']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # Linear Regression\n","    modelLR = LinearRegression()\n","    modelLR.fit(X_train, y_train)\n","    y_pred = modelLR.predict(X_test)\n","    maeLR = mean_absolute_error(y_test, y_pred)\n","    mseLR = mean_squared_error(y_test, y_pred)\n","\n","    # Random Forest\n","    modelRF = RandomForestRegressor(random_state=42)\n","    modelRF.fit(X_train, y_train)\n","    y_pred = modelRF.predict(X_test)\n","    maeRF = mean_absolute_error(y_test, y_pred)\n","    mseRF = mean_squared_error(y_test, y_pred)\n","    \n","    # XGBoost\n","    modelXGB = XGBRegressor()\n","    modelXGB.fit(X_train, y_train)\n","    y_pred = modelXGB.predict(X_test)\n","    maeXGB = mean_absolute_error(y_test, y_pred)\n","    mseXGB = mean_squared_error(y_test, y_pred)\n","\n","    print(\"{:<12} \\nLinear Regression MAE: {:<5} MSE: {:<6} Random Forest MAE: {:<5} MSE: {:<6} XGBoost MAE: {:<5} MSE: {:<6}\\n\".format(i, round(maeLR, 2), round(mseLR, 2), round(maeRF, 2), round(mseRF, 2), round(maeXGB, 2), round(mseXGB, 2)))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":3947402,"sourceId":7173504,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
